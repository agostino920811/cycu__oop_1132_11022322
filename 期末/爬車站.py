import asyncio
import csv
import re
from playwright.async_api import async_playwright, Browser, Page

# --- 1. ä½¿ç”¨ Playwright ç²å–æ‰€æœ‰å…¬è»Šè·¯ç·šåˆ—è¡¨ (åªåœ¨ç¨‹å¼å•Ÿå‹•æ™‚åŸ·è¡Œä¸€æ¬¡) ---
async def fetch_all_bus_routes(page: Page):
    """
    ä½¿ç”¨ Playwright å¾å°åŒ—å¸‚å…¬è»Šå‹•æ…‹è³‡è¨Šç³»çµ±ç²å–æ‰€æœ‰å…¬è»Šè·¯ç·šåç¨±å’Œ route_idã€‚
    é€™å€‹å‡½æ•¸æ‡‰è©²åªåœ¨ç¨‹å¼å•Ÿå‹•æ™‚åŸ·è¡Œä¸€æ¬¡ã€‚
    """
    print("æ­£åœ¨ç²å–æ‰€æœ‰å…¬è»Šè·¯ç·šåˆ—è¡¨ï¼Œè«‹ç¨å€™...")
    all_routes = []
    try:
        await page.goto("https://ebus.gov.taipei/ebus", wait_until='domcontentloaded', timeout=60000)

        await page.wait_for_selector("a[data-toggle='collapse'][href*='#collapse']", state='attached', timeout=30000)

        collapse_links = await page.query_selector_all("a[data-toggle='collapse'][href*='#collapse']")

        click_tasks = []
        for link in collapse_links:
            if await link.get_attribute("aria-expanded") == "false" or not await link.get_attribute("aria-expanded"):
                click_tasks.append(link.click())

        if click_tasks:
            await asyncio.gather(*click_tasks)
            await asyncio.sleep(1)

        await page.wait_for_selector("a[href*='javascript:go']", timeout=15000)
        bus_links = await page.query_selector_all("a[href*='javascript:go']")
        for link in bus_links:
            href = await link.get_attribute("href")
            name = await link.inner_text()
            if href and name:
                route_id_match = re.search(r"go\('([^']+)'\)", href)
                if route_id_match:
                    route_id = route_id_match.group(1)
                    all_routes.append({"name": name.strip(), "route_id": route_id})
    except Exception as e:
        print(f"éŒ¯èª¤ï¼šç„¡æ³•ç²å–å…¬è»Šè·¯ç·šåˆ—è¡¨ã€‚åŸå› ï¼š{e}")
    print(f"å·²ç²å– {len(all_routes)} æ¢å…¬è»Šè·¯ç·šã€‚")
    return all_routes

# --- 2. ä½¿ç”¨ Playwright ç²å–æŒ‡å®šè·¯ç·šçš„ç«™ç‰Œè©³æƒ… (ç„¡é ä¼°æ™‚é–“ï¼Œå› ç‚ºæ˜¯éœæ…‹çˆ¬å–) ---
async def fetch_bus_stops_details(page: Page, route_id: str):
    """
    ä½¿ç”¨ Playwright å¾å°åŒ—å¸‚å…¬è»Šå‹•æ…‹è³‡è¨Šç³»çµ±æŠ“å–æŒ‡å®šè·¯ç·šçš„ç«™ç‰Œåç¨±ã€ç¶“ç·¯åº¦ã€IDã€åºè™Ÿã€æ–¹å‘ã€‚
    æ­¤å‡½æ•¸ç”¨æ–¼éœæ…‹çˆ¬å–æ‰€æœ‰ç«™é»ä¿¡æ¯ï¼Œä¸åŒ…å«å¯¦æ™‚é ä¼°æ™‚é–“ã€‚
    è¿”å›ä¸€å€‹åŒ…å«æ‰€æœ‰ç«™ç‰Œè©³ç´°ä¿¡æ¯çš„åˆ—è¡¨ã€‚
    """
    url = f"https://ebus.gov.taipei/Route/StopsOfRoute?routeid={route_id}"
    all_stops_data = []

    try:
        await page.goto(url, wait_until='domcontentloaded', timeout=30000)
        await page.wait_for_selector("p.stationlist-come-go-c", timeout=15000)

        # --- æŠ“å–å»ç¨‹ç«™é» ---
        go_button = await page.query_selector("a.stationlist-go")
        if go_button:
            await go_button.click()
            await page.wait_for_selector("#GoDirectionRoute li .auto-list-stationlist", timeout=10000)
            go_elements = await page.query_selector_all("#GoDirectionRoute li .auto-list-stationlist")
            for element in go_elements:
                stop_info = await extract_stop_details_static(element, "å»ç¨‹")
                if stop_info:
                    all_stops_data.append(stop_info)

        # --- æŠ“å–è¿”ç¨‹ç«™é» ---
        return_button = await page.query_selector("a.stationlist-come")
        if return_button:
            await return_button.click()
            await page.wait_for_selector("#BackDirectionRoute li .auto-list-stationlist", timeout=10000)
            return_elements = await page.query_selector_all("#BackDirectionRoute li .auto-list-stationlist")
            for element in return_elements:
                stop_info = await extract_stop_details_static(element, "è¿”ç¨‹")
                if stop_info:
                    all_stops_data.append(stop_info)

    except Exception as e:
        print(f"[éŒ¯èª¤] ç²å–è·¯ç·š {route_id} ç«™ç‰Œæ•¸æ“šå¤±æ•—ï¼š{e}")

    # No need for unique_stops_map here as deduplication will happen in main_crawler
    # Just sort for consistency
    sorted_stops_data = sorted(all_stops_data, key=lambda x: (x.get('direction', ''), x.get('sequence', 0)))

    return sorted_stops_data

async def extract_stop_details_static(element, direction):
    """
    å¾ Playwright çš„å…ƒç´ ä¸­æå–ç«™ç‰Œçš„è©³ç´°è³‡è¨Šï¼ˆç”¨æ–¼éœæ…‹çˆ¬å–ï¼‰ã€‚
    """
    name_elem = await element.query_selector(".auto-list-stationlist-place")
    number_elem = await element.query_selector(".auto-list-stationlist-number")
    stop_id_elem = await element.query_selector("input[name='item.UniStopId']")
    latitude_elem = await element.query_selector("input[name='item.Latitude']")
    longitude_elem = await element.query_selector("input[name='item.Longitude']")

    name_text = await name_elem.inner_text() if name_elem else "æœªçŸ¥ç«™å"
    number_text = await number_elem.inner_text() if number_elem else "æœªçŸ¥åºè™Ÿ"
    stop_id_value = await stop_id_elem.get_attribute("value") if stop_id_elem else "æœªçŸ¥ç·¨è™Ÿ"
    latitude_value = await latitude_elem.get_attribute("value") if latitude_elem else "æœªçŸ¥ç·¯åº¦"
    longitude_value = await longitude_elem.get_attribute("value") if longitude_elem else "æœªçŸ¥ç¶“åº¦"

    try:
        lat = float(latitude_value)
        lon = float(longitude_value)
    except ValueError:
        lat = None
        lon = None
        return None

    return {
        "direction": direction,
        "sequence": int(number_text.strip()) if number_text.strip().isdigit() else None,
        "name": name_text.strip(),
        "stop_id": stop_id_value.strip(),
        "lat": lat,
        "lon": lon,
        "estimated_time": "N/A" # åœ¨éœæ…‹çˆ¬å–ä¸­ï¼Œé ä¼°æ™‚é–“æ˜¯ç„¡æ•ˆçš„
    }

# --- 3. å°‡æ‰€æœ‰ç«™ç‰Œæ•¸æ“šè¼¸å‡ºç‚º CSV æª”æ¡ˆ ---
def export_all_stops_to_csv(all_data):
    """
    å°‡æ‰€æœ‰å…¬è»Šè·¯ç·šçš„æ‰€æœ‰ç«™ç‰Œæ•¸æ“šè¼¸å‡ºç‚ºå–®ä¸€ CSV æª”æ¡ˆã€‚
    """
    csv_filename = "all_taipei_bus_stops.csv"
    try:
        with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['è·¯ç·šåç¨±', 'è·¯ç·šID', 'æ–¹å‘', 'ç«™åº', 'ç«™ç‰Œåç¨±', 'ç«™ç‰ŒID', 'ç·¯åº¦', 'ç¶“åº¦', 'é ä¼°æ™‚é–“']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

            writer.writeheader()
            for row in all_data:
                writer.writerow(row)
        print(f"\nâœ… æ‰€æœ‰å…¬è»Šè·¯ç·šç«™ç‰Œæ•¸æ“šå·²æˆåŠŸè¼¸å‡ºåˆ° '{csv_filename}'ã€‚")
    except Exception as e:
        print(f"éŒ¯èª¤ï¼šè¼¸å‡º '{csv_filename}' æ™‚ç™¼ç”Ÿå•é¡Œï¼š{e}")

# --- ä¸»çˆ¬èŸ²ç¨‹å¼ ---
async def main_crawler():
    print("ğŸš€ æ­£åœ¨å•Ÿå‹•å°åŒ—å¸‚å…¬è»Šå…¨è·¯ç·šç«™ç‰Œæ•¸æ“šçˆ¬å–ç¨‹åº...")
    all_collected_stops = []
    # Use a set to store unique keys of already added stops to prevent duplicates
    # Key format: (route_id, direction, stop_id, sequence)
    processed_stop_keys = set()

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        all_bus_routes = await fetch_all_bus_routes(page)

        total_routes = len(all_bus_routes)
        for i, route in enumerate(all_bus_routes):
            route_name = route['name']
            route_id = route['route_id']
            print(f"[{i+1}/{total_routes}] æ­£åœ¨çˆ¬å–è·¯ç·š: {route_name} (ID: {route_id})...")
            stops_details = await fetch_bus_stops_details(page, route_id)

            for stop in stops_details:
                # Create a unique key for the stop based on route, direction, stop_id, and sequence
                # This ensures we don't add the same physical stop on the same route/direction twice
                unique_key = (route_id, stop.get('direction'), stop.get('stop_id'), stop.get('sequence'))

                if unique_key not in processed_stop_keys:
                    row = {
                        'è·¯ç·šåç¨±': route_name,
                        'è·¯ç·šID': route_id,
                        'æ–¹å‘': stop.get('direction', ''),
                        'ç«™åº': stop.get('sequence', ''),
                        'ç«™ç‰Œåç¨±': stop.get('name', ''),
                        'ç«™ç‰ŒID': stop.get('stop_id', ''),
                        'ç·¯åº¦': stop.get('lat', ''),
                        'ç¶“åº¦': stop.get('lon', ''),
                        'é ä¼°æ™‚é–“': stop.get('estimated_time', '')
                    }
                    all_collected_stops.append(row)
                    processed_stop_keys.add(unique_key) # Add the key to the set
            await asyncio.sleep(0.1) # Short delay to be polite to the server

        await browser.close()

    # Final check for duplicates based on the CSV row's core identifying columns
    # This is a safeguard, but the set-based deduplication above should handle most cases.
    # If pandas is available, this would be more robust:
    # import pandas as pd
    # df = pd.DataFrame(all_collected_stops)
    # df.drop_duplicates(subset=['è·¯ç·šåç¨±', 'è·¯ç·šID', 'æ–¹å‘', 'ç«™åº', 'ç«™ç‰ŒID'], inplace=True)
    # all_collected_stops = df.to_dict('records')

    export_all_stops_to_csv(all_collected_stops)
    print("\nğŸ‰ çˆ¬å–ä»»å‹™å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main_crawler())